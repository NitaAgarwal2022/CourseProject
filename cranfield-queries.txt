CS410 MP1 -- Getting Familiar with Text  Due Sept 4, 2022:    For students who just added the class, NO LATE PENALTY will be applied to submissions before Sept 11    In this assignment, you will perform your first text mining analysis with the MeTA toolkit. We will first provide some instructions on setting up and exploring basic functionalities available in MeTA. Details on the graded portion of the assignment can be found at the bottom (Trying it out on your own! section)    Setup  We'll use metapy---Python bindings for MeTA. , use the following commands to get started.    Please note that students have had issues using metapy with specific Python versions in the past (e.g. Python 3.7 on mac). To avoid issues, please use Python 2.7 or 3.5. Your code will be tested using Python 3.5    # Ensure your pip is up to date  pip install --upgrade pip    # install metapy!  pip install metapy pytoml  Start  Let's start by importing metapy. Open a terminal and type    python  to get started    #import the MeTA python bindings  import metapy  #If you'd like, you can tell MeTA to log to stderr so you can get progress output when running long-running function calls.  metapy.log_to_stderr()  Now, let's create a document with some content to experiment on    doc = metapy.index.Document()  doc.content(I said that I can't believe that it only costs $19.95!)  Tokenization  MeTA provides a stream-based interface for performing document tokenization. Each stream starts off with a Tokenizer object, and in most cases you should use the Unicode standard aware ICUTokenizer.    tok = metapy.analyzers.ICUTokenizer()  Tokenizers operate on raw text and provide an Iterable that spits out the individual text tokens. Let's try running just the ICUTokenizer to see what it does.    tok.set_content(doc.content()) # this could be any string  tokens = [token for token in tok]  print(tokens)  One thing that you likely immediately notice is the insertion of these pseudo-XML looking tags. These are called sentence boundary tags. As a side-effect, a default-construted ICUTokenizer discovers the sentences in a document by delimiting them with the sentence boundary tags. Let's try tokenizing a multi-sentence document to see what that looks like.    doc.content(I said that I can't believe that it only costs $19.95! I could only find it for more than $30 before.)  tok.set_content(doc.content())  tokens = [token for token in tok]  print(tokens)  Most of the information retrieval techniques you have likely been learning about in this class don't need to concern themselves with finding the boundaries between separate sentences in a document, but later today we'll explore a scenario where this might matter more. Let's pass a flag to the ICUTokenizer constructor to disable sentence boundary tags for now.    tok = metapy.analyzers.ICUTokenizer(suppress_tags=True)  tok.set_content(doc.content())  tokens = [token for token in tok]  print(tokens)  As mentioned earlier, MeTA treats tokenization as a streaming process, and that it starts with a tokenizer. It is often beneficial to modify the raw underlying tokens of a document, and thus change its representation. The intermediate steps in the tokenization stream are represented with objects called Filters. Each filter consumes the content of a previous filter (or a tokenizer) and modifies the tokens coming out of the stream in some way. Let's start by using a simple filter that can help eliminate a lot of noise that we might encounter when tokenizing web documents: a LengthFilter.    tok = metapy.analyzers.LengthFilter(tok, min=2, max=30)  tok.set_content(doc.content())  tokens = [token for token in tok]  print(tokens)  Here, we can see that the LengthFilter is consuming our original ICUTokenizer. It modifies the token stream by only emitting tokens that are of a minimum length of 2 and a maximum length of 30. This can get rid of a lot of punctuation tokens, but also excessively long tokens such as URLs.    Stopword removal and stemming  Another common trick is to remove stopwords. In MeTA, this is done using a ListFilter.    wget -nc https://raw.githubusercontent.com/meta-toolkit/meta/master/data/lemur-stopwords.txt  Note: wget is a command to download files from links. Another simpler option is to open a web browser, type the link on the address bar and download the file manually    tok = metapy.analyzers.ListFilter(tok, lemur-stopwords.txt, metapy.analyzers.ListFilter.Type.Reject)  tok.set_content(doc.content())  tokens = [token for token in tok]  print(tokens)  Here we've downloaded a common list of stopwords and created a ListFilter to reject any tokens that occur in that list of words. You can see how much of a difference removing stopwords can make on the size of a document's token stream!    Another common filter that people use is called a stemmer, or lemmatizer. This kind of filter tries to modify individual tokens in such a way that different inflected forms of a word all reduce to the same representation. This lets you, for example, find documents about a run when you search running or runs. A common stemmer is the Porter2 Stemmer, which MeTA has an implementation of. Let's try it!    tok = metapy.analyzers.Porter2Filter(tok)  tok.set_content(doc.content())  tokens = [token for token in tok]  print(tokens)  N-grams  Finally, after you've got the token stream configured the way you'd like, it's time to analyze the document by consuming each token from its token stream and performing some actions based on these tokens. In the simplest case, our action can simply be counting how many times these tokens occur. For clarity, let's switch back to a simpler token stream first. We will write a token stream that tokenizes with ICUTokenizer, and then lowercases each token.    tok = metapy.analyzers.ICUTokenizer(suppress_tags=True)  tok = metapy.analyzers.LowercaseFilter(tok)  tok.set_content(doc.content())  tokens = [token for token in tok]  print(tokens)  Now, let's count how often each individual token appears in the stream. This representation is called bag of words representation or unigram word counts. In MeTA, classes that consume a token stream and emit a document representation are called Analyzers.    ana = metapy.analyzers.NGramWordAnalyzer(1, tok)  print(doc.content())  unigrams = ana.analyze(doc)  print(unigrams)  If you noticed the name of the analyzer, you might have realized that you can count not just individual tokens, but groups of them. Unigram means 1-gram, and we count individual tokens. Bigram means 2-gram, and we count adjacent tokens together as a group. Let's try that now.    ana = metapy.analyzers.NGramWordAnalyzer(2, tok)  bigrams = ana.analyze(doc)  print(bigrams)  Now the individual tokens we're counting are pairs of tokens. Sometimes looking at n-grams of characters is useful.    tok = metapy.analyzers.CharacterTokenizer()  ana = metapy.analyzers.NGramWordAnalyzer(4, tok)  fourchar_ngrams = ana.analyze(doc)  print(fourchar_ngrams)  POS tagging  Now, let's explore something a little bit different. MeTA also has a natural language processing (NLP) component, which currently supports two major NLP tasks: part-of-speech tagging and syntactic parsing. POS tagging is a task in NLP that involves identifying a type for each word in a sentence. For example, POS tagging can be used to identify all of the nouns in a sentence, or all of the verbs, or adjectives, or… This is useful as first step towards developing an understanding of the meaning of a particular sentence. MeTA places its POS tagging component in its sequences library. Let's play with some sequences first to get an idea of how they work. We'll start of by creating a sequence.    seq = metapy.sequence.Sequence()  Now, we can add individual words to this sequence. Sequences consist of a list of Observations, which are essentially (word, tag) pairs. If we don't yet know the tags for a Sequence, we can just add individual words and leave the tags unset. Words are called symbols in the library terminology.    for word in [The, dog, ran, across, the, park, .]:      seq.add_symbol(word)    print(seq)  The printed form of the sequence shows that we do not yet know the tags for each word. Let's fill them in by using a pre-trained POS-tagger model that's distributed with MeTA.    wget -nc https://github.com/meta-toolkit/meta/releases/download/v3.0.1/greedy-perceptron-tagger.tar.gz  tar xvf greedy-perceptron-tagger.tar.gz  tagger = metapy.sequence.PerceptronTagger(perceptron-tagger/)  tagger.tag(seq)  print(seq)  Each tag indicates the type of a word, and this particular tagger was trained to output the tags present in the Penn Treebank tagset. But what if we want to POS-tag a document?    doc = metapy.index.Document()  doc.content(I said that I can't believe that it only costs $19.95!)  tok = metapy.analyzers.ICUTokenizer() # keep sentence boundaries!  tok = metapy.analyzers.PennTreebankNormalizer(tok)  tok.set_content(doc.content())  tokens = [token for token in tok]  print(tokens)  Now, we will write a function that can take a token stream that contains sentence boundary tags and returns a list of Sequence objects. We will not include the sentence boundary tags in the actual Sequence objects.    def extract_sequences(tok):      sequences = []      for token in tok:          if token == '<s>':              sequences.append(metapy.sequence.Sequence())          elif token != '</s>':              sequences[-1].add_symbol(token)      return sequences    doc = metapy.index.Document()  doc.content(I said that I can't believe that it only costs $19.95!)  tok.set_content(doc.content())  for seq in extract_sequences(tok):      tagger.tag(seq)      print(seq)  Config.toml file: setting up a pipeline  In practice, it is often beneficial to combine multiple feature sets together. We can do this with a MultiAnalyzer. Let's combine unigram words, bigram POS tags, and rewrite rules for our document feature representation. We can certainly do this programmatically, but doing so can become tedious quite quickly. Instead, let's use MeTA's configuration file format to specify our analyzer, which we can then load in one line of code. MeTA uses TOML configuration files for all of its configuration. If you haven't heard of TOML before, don't panic! It's a very simple, readable format. Open a text editor and copy the text below, but be careful not to modify the contents. Save it as config.toml .    #Add this as a config.toml file to your project directory  stop-words = lemur-stopwords.txt    [[analyzers]]  method = ngram-word  ngram = 1  filter = default-unigram-chain    [[analyzers]]  method = ngram-pos  ngram = 2  filter = [{type = icu-tokenizer}, {type = ptb-normalizer}]  crf-prefix = crf    [[analyzers]]  method = tree  filter = [{type = icu-tokenizer}, {type = ptb-normalizer}]  features = [subtree]  tagger = perceptron-tagger/  parser = parser/  Each [[analyzers]] block defines another analyzer to combine for our feature representation. Since ngram-word is such a common analyzer, we have defined some default filter chains that can be used with shortcuts. default-unigram-chain is a filter chain suitable for unigram words; default-chain is a filter chain suitable for bigram words and above.    To run this example, we will need to download some additional MeTA resources:    wget -nc https://github.com/meta-toolkit/meta/releases/download/v3.0.2/crf.tar.gz  tar xvf crf.tar.gz  wget -nc https://github.com/meta-toolkit/meta/releases/download/v3.0.2/greedy-constituency-parser.tar.gz  tar xvf greedy-constituency-parser.tar.gz  We can now load an analyzer from this configuration file:    ana = metapy.analyzers.load('config.toml')  doc = metapy.index.Document()  doc.content(I said that I can't believe that it only costs $19.95!)  print(ana.analyze(doc))  Trying it out on your own!  Finally, let's test whether you can do such analysis on your own! Inside this repository, you will find example.py where we ask you to fill in your code. You are required to create a function that tokenizes with ICUTokenizer (without the end/start tags, i.e. use the argument suppress_tags=True), lowercases, removes words with less than 2 and more than 5 characters, performs stemming and produces trigrams for an input sentence. Once you edit the example.py to fill in the function, you can check whether your submission passed the tests.
Overview of CS410 MP2  MP2 is a 4-part assignment in which you will get familiar with building and evaluating Search Engines. Following our philosophy of promoting collaborative learning, we have designed this comprehensive MP with the goal of not only teaching you useful skills of building and evaluating search engines, but also allowing all of you to leverage your MP2 work to collaboratively build a useful Digital Library (DL) search engine for CS410, which you all can you use to enrich your learning experience. You will be able to use the CS410 DL search engine to easily locate course information.   The actual content the search engine supports would depend on what you all would save to the DL and thus will naturally evolve to include increasingly more content as we all use it over time in the course. The search engine you will build will also serve as a basis for you to design your course projects, which can then further improve the utility of such a CS410 DL system, providing more benefit to all of us as well as many students of CS410 in the future.  Specifically, the four parts of MP will systematically expose you to four major tasks involved in building and evaluating a search engine:  MP2.1 covers content acquisition and creation of a collection of searchable documents. The outcome of MP2.1 is a collection of documents relevant to CS410.  MP2.2 covers annotation of the collection created in MP2.1 to create a set of queries and the corresponding relevance judgments. The outcome of MP2.2 is a test set that can be used to quantitatively evaluate a search engine algorithm.  MP2.3 Implementation and evaluation of a baseline ranking algorithm, where you will be asked to complete the implementation of a state of the art ranking algorithm using the MeTA toolkit.  MP2.4 Improvement of ranking algorithms, where you will have an opportunity to freely explore any ideas to vary/improve over the baseline ranking algorithm(s). A leaderboard-based competition (evaluated using the test set you will build from MP2.2) will be held to encourage you to propose new ideas to design an effective ranking algorithm for ranking CS410 content pages. Your new algorithms can then be used to power the CS410 DL search engine and potentially improve its utility for all of us.  To enable such a massive collaborative MP, we will leverage an existing baseline CS410 Digital Library system that our TA, Kevin Ros, has built, which consists of a search engine and a  browser (Chrome) extension. The Chrome extension enables you to easily save any pages from the Web that are potentially relevant/useful to CS410 with an explanation of how it is relevant or useful. It also enables you to conveniently take notes from any webpage by highlighting the content and saving it to the CS410 DL. The search engine enables you to search over the collection of the saved pages made by all the students in CS410 and has the ability of updating the collection in real time, meaning that once you save a page, it would be immediately searchable via this search engine.  With this baseline CS410DL system in place, you will be able to finish MP2.1 and MP2.2 conveniently without spending any extra time for submission to Coursera or LiveDataLab. Specifically, the baseline CS410DL system also supports user accounts and can log user activities, thus grading can be done automatically based on the pages you have saved and relevance judgments you have made via the system. This means that you do not need to make any conventional submission specifically for MP2.1 or MP2.2; you just need to follow the instructions to use the system to save useful content and make relevance judgments for some sample queries that you are interested in. MP2.1  Your task for MP2.1 is to use the baseline CS410DL system (briefly described above) to save a certain number of web pages so as to help create a large collection of relevant online documents to CS410 content for building a useful CS410 Digital Library. More information about this baseline CS410DL system (including installation instructions and the URL to the search engine) can be found in this Google document.  Specifically, please follow the instructions below to complete MP2.1:  Follow the extension installation instructions in the Google document.  Become familiar with the extension, the website, and the general functionality.  Create an account using your NetID and university email. Please make sure to write down your password! (The system at this point cannot help you reset your password.) Using the correct NetID / email is essential for grading.  Use the Submit to CS410 DL functionality to record at least 15 web pages that you found useful while browsing material for CS410 or relevant content on the Web. This could be lecture points, papers, campuswire posts, videos, etc. We encourage you to explore multiple uses of the CS410DL system, ideally spend a few minutes to explore each of the following categories:  Bookmark some useful course information pages (e.g., some pages/lectures on Coursera or some useful posts on campuswire)  Save some relevant research papers or technical articles related to CS410 (e.g., you can do a search in Google Scholar with any interesting keywords); please make sure to highlight a lot of relevant content in those articles to enable CS410DL to match them with a query (think about using CS410DL to conveniently take notes from any paper or technical article and share your notes with your classmates)  Find some interesting relevant videos on Youtube; again, you can search in Youtube with interesting keywords  Find some relevant development resources related to CS410 such as useful toolkits or online tools; you can search with appropriate keywords in Google  Find some pages about startups using technologies related to CS410 (e.g., using a query like text mining startups to search in Google).  In general, please try to provide an informative description of any saved page and/or highlight relevant content whenever possible; the extra text helps CS410DL work better as a search engine since at this point, it only matches a query with the text you typed in or highlighted (this enables high relevance whenever there is a match).  Your submitted content should be available immediately for everyone in CS410 to search for on the search engine website. And grading for the assignment will be updated on Coursera once the assignment is due.  MP2.1 should be completed by Sept 11, 2022 at 11:59 pm (CDT).
Overview of CS410 MP2  MP2 is a 4-part assignment in which you will get familiar with building and evaluating Search Engines. Following our philosophy of promoting collaborative learning, we have designed this comprehensive MP with the goal of not only teaching you useful skills of building and evaluating search engines, but also allowing all of you to leverage your MP2 work to collaboratively build a useful Digital Library (DL) search engine for CS410, which you all can you use to enrich your learning experience. You will be able to use the CS410 DL search engine to easily locate course information.   The actual content the search engine supports would depend on what you all would save to the DL and thus will naturally evolve to include increasingly more content as we all use it over time in the course. The search engine you will build will also serve as a basis for you to design your course projects, which can then further improve the utility of such a CS410 DL system, providing more benefit to all of us as well as many students of CS410 in the future.  Specifically, the four parts of MP will systematically expose you to four major tasks involved in building and evaluating a search engine:  MP2.1 covers content acquisition and creation of a collection of searchable documents. The outcome of MP2.1 is a collection of documents relevant to CS410.  MP2.2 covers annotation of the collection created in MP2.1 to create a set of queries and the corresponding relevance judgments. The outcome of MP2.2 is a test set that can be used to quantitatively evaluate a search engine algorithm.  MP2.3 Implementation and evaluation of a baseline ranking algorithm, where you will be asked to complete the implementation of a state of the art ranking algorithm using the MeTA toolkit.  MP2.4 Improvement of ranking algorithms, where you will have an opportunity to freely explore any ideas to vary/improve over the baseline ranking algorithm(s). A leaderboard-based competition (evaluated using the test set you will build from MP2.2) will be held to encourage you to propose new ideas to design an effective ranking algorithm for ranking CS410 content pages. Your new algorithms can then be used to power the CS410 DL search engine and potentially improve its utility for all of us.  To enable such a massive collaborative MP, we will leverage an existing baseline CS410 Digital Library system that our TA, Kevin Ros, has built, which consists of a search engine and a  browser (Chrome) extension. The Chrome extension enables you to easily save any pages from the Web that are potentially relevant/useful to CS410 with an explanation of how it is relevant or useful. It also enables you to conveniently take notes from any webpage by highlighting the content and saving it to the CS410 DL. The search engine enables you to search over the collection of the saved pages made by all the students in CS410 and has the ability of updating the collection in real time, meaning that once you save a page, it would be immediately searchable via this search engine.  With this baseline CS410DL system in place, you will be able to finish MP2.1 and MP2.2 conveniently without spending any extra time for submission to Coursera or LiveDataLab. Specifically, the baseline CS410DL system also supports user accounts and can log user activities, thus grading can be done automatically based on the pages you have saved and relevance judgments you have made via the system. This means that you do not need to make any conventional submission specifically for MP2.1 or MP2.2; you just need to follow the instructions to use the system to save useful content and make relevance judgments for some sample queries that you are interested in.   MP2.2  For MP2.2, you will use the CS410 DL Search Engine website to submit relevance judgments for a given query. Upon searching a query, you should see something like this (the UI will be updated mid-morning on 9/12):    Your job is to examine the search results and provide your relevance judgments (i.e., if a result answers your query). After you select Relevant or Not Relevant for the returned results, you should click Submit Relevance Judgments which will notify you of the status of the submission.    For full credit, you must submit at least 5 queries, each with at least 10 relevance judgments (or up to the number of returned results). That is, for each different query, you mark relevant/not relevant for at least ten results. The judged results do not need to be contiguous.     We ask that you avoid trivial queries. Specific queries like intuitive videos of bm25 or startups doing nlp are encouraged. Try to come up with queries that reflect your actual information needs, so that reading and judging results can also help you better understand the course material.    Grading for this assignment will be updated on Coursera once the assignment is due. You should be able to track the number of submitted judgments on the Your Submissions page.  If you have at least five judgments, you should receive full credit.    Note that you do not need to use LiveDataLab or submit anything on GitHub. Everything for MP2.2 is done via the CS410 Digital Library.    MP2.2 should be completed by Sept 18, 2022 at 11:59 pm (CDT).          
CS410 MP2---Search Engines    In this 4-part MP, you will get familiar with building and evaluating Search Engines.    ## Part 3    ### Due: Sept 18, 2022    In this part, you will use the MeTA toolkit to do the following:  - create a search engine over a dataset  - investigate the effect of parameter values for a standard retrieval function  - write the InL2 retrieval function  - investigate the effect of the parameter value for InL2      Also, you are free to edit all files **except**   - livedatalab_config.json      ## Setup  We'll use [metapy](https://github.com/meta-toolkit/metapy)---Python bindings for MeTA.   If you have not installed metapy so far, use the following commands to get started.    ```bash  # Ensure your pip is up to date  pip install --upgrade pip    # install metapy!  pip install metapy pytoml  ```    Read the [C++ Search Tutorial](https://meta-toolkit.org/search-tutorial.html). Read *Initially setting up the config file and Relevance judgements*.  Read the [python Search Tutorial](https://github.com/meta-toolkit/metapy/blob/master/tutorials/2-search-and-ir-eval.ipynb)    We have provided the following files:  - Cranfield dataset in MeTA format.  - cranfield-queries.txt: Queries one per line  - cranfield-qrels.txt: Relevance judgements for the queries  - stopwords.txt: A file containing stopwords that will not be indexed.  - config.toml: A config file with paths set to all the above files, including index and ranker settings.    ## Indexing the data  To index the data using metapy, use the following .  ```python  import metapy  idx = metapy.index.make_inverted_index('config.toml')  ```    ## Search the index  You can examine the data inside the cranfield directory to get a sense about the dataset and the queries.    To examine the index we built from the previous section. You can use metapy's functions.    ```python  # Examine number of documents  idx.num_docs()  # Number of unique terms in the dataset  idx.unique_terms()  # The average document length  idx.avg_doc_length()  # The total number of terms  idx.total_corpus_terms()  ```    Here is a list of all the rankers in MeTA.Viewing the class comment in the header files shows the optional parameters you can set in the config file:    - [Okapi BM25](https://github.com/meta-toolkit/meta/blob/master/include/meta/index/ranker/okapi_bm25.h), method = **bm25**   - [Pivoted Length Normalization](https://github.com/meta-toolkit/meta/blob/master/include/meta/index/ranker/pivoted_length.h), method = **pivoted-length**  - [Absolute Discount Smoothing](https://github.com/meta-toolkit/meta/blob/master/include/meta/index/ranker/absolute_discount.h), method = **absolute-discount**  - [Jelinek-Mercer Smoothing](https://github.com/meta-toolkit/meta/blob/master/include/meta/index/ranker/jelinek_mercer.h), method = **jelinek-mercer**  - [Dirichlet Prior Smoothing](https://github.com/meta-toolkit/meta/blob/master/include/meta/index/ranker/dirichlet_prior.h), method = **dirichlet-prior**    In metapy, the rankers can be called as:    ```python  metapy.index.OkapiBM25(k1, b, k3) where k1, b, k3 are function arguments, e.g. ranker = metapy.index.OkapiBM25(k1=1.2,b=0.75,k3=500)  metapy.index.PivotedLength(s)   metapy.index.AbsoluteDiscount(delta)  metapy.index.JelinekMercer(lambda)  metapy.index.DirichletPrior(mu)  ```    ## Varying a parameter  Choose one of the above retrieval functions and one of its parameters (don’t choose BM25 + k3, it’s not interesting). For example, you could choose Dirichlet Prior and mu.    Change the **ranker** to your method and parameters. In the example, it is set to **bm25**. Use at least 10 different values for the parameter you chose; try to choose the values such that you can find a maximum MAP.    Here's a tutorial on how to do an evaluation of your parameter setting (this code is included in *search_eval.py*):      ```python  # Build the query object and initialize a ranker  query = metapy.index.Document()  ranker = metapy.index.OkapiBM25(k1=1.2,b=0.75,k3=500)  # To do an IR evaluation, we need to use the queries file and relevance judgements.  ev = metapy.index.IREval('config.toml')  # Load the query_start from config.toml or default to zero if not found  with open('config.toml', 'r') as fin:          cfg_d = pytoml.load(fin)  query_cfg = cfg_d['query-runner']  query_start = query_cfg.get('query-id-start', 0)  # We will loop over the queries file and add each result to the IREval object ev.  num_results = 10  with open('cranfield-queries.txt') as query_file:      for query_num, line in enumerate(query_file):          query.content(line.strip())          results = ranker.score(idx, query, num_results)                                      avg_p = ev.avg_p(results, query_start + query_num, num_results)          print(Query {} average precision: {}.format(query_num + 1, avg_p))  ev.map()  ```    ## Writing InL2    You will now implement a retrieval function called InL2. It is described in [this](http://dl.acm.org/citation.cfm?id=582416) paper:   For this assignment, we will only concern ourselves with writing the function and not worry about its derivation.   InL2 is formulated as     ![image](https://drive.google.com/uc?export=view&id=1_Q2CTMe6o2RP9PGf8HPsggai9LVyVmEU)     Please use this link if the image does not display: https://drive.google.com/uc?export=view&id=1_Q2CTMe6o2RP9PGf8HPsggai9LVyVmEU      , where    ![image](https://drive.google.com/uc?export=view&id=1gcbywLx0ZEU3eqxlDtLk6o4Yxd788IiK)    Please use this link if the image does not display: https://drive.google.com/uc?export=view&id=1gcbywLx0ZEU3eqxlDtLk6o4Yxd788IiK    It uses the following variables:    - <em> Q,D,t </em> : the current query, document, and term  - <em> N </em> : the total number of documents in the corpus C  - <em> avgdl </em> : the average document length  - <em> c > 0 </em> : is a parameter    Determine if this function captures the TF, IDF, and document length normalization properties. Where (if anywhere) are they represented in the formula? You don’t need to submit your answers.    To implement InL2, define your own ranking function in Python, as shown below.   You do not need to create a new file, the template is included in *search_eval.py*  You will need to modify the function **score_one**.   Do not forget to call the InL2 ranker by editing the return statement of *load_ranker* function inside search_eval.py.    The parameter to the function is a score_data sd object. See the object [here](https://github.com/meta-toolkit/meta/blob/master/include/meta/index/score_data.h).    As you can see, the sd variable contains all the information you need to write the scoring function. The function you’re writing represents one term in the large InL2 sum.    ```python  class InL2Ranker(metapy.index.RankingFunction):                                                                                                                                  Create a new ranking function in Python that can be used in MeTA.                                                                                                   def __init__(self, some_param=1.0):                                                       self.param = some_param          # You *must* call the base class constructor here!          super(InL2Ranker, self).__init__()                                                                                                                                 def score_one(self, sd):                    You need to override this function to return a score for a single term.          For fields available in the score_data sd object,          @see https://meta-toolkit.org/doxygen/structmeta_1_1index_1_1score__data.html                    return (self.param + sd.doc_term_count) / (self.param * sd.doc_unique_terms + sd.doc_size)  ```      ## Varying InL2’s parameter  Perform the same parameter analysis with InL2’s <em> c </em> parameter.     ## Statistical significance testing    Modifying the code in Varying a parameter section, you can create a file with average precision data.     Use BM25 as a ranker and create a file called bm25.avg_p.txt.     Then use your ranker InL2 and create a file called inl2.avg_p.txt.     Each of these files is simply a list of the APs from the queries.    We want to test whether the difference between your two optimized retrieval functions is statistically significant.    If you’re using R, you can simply do    ```R  bm25 = read.table('bm25.avg_p.txt')$V1  inl2 = read.table('inl2.avg_p.txt')$V1  t.test(bm25, inl2, paired=T)  ```    You don’t have to use R; you can even write a script to calculate the answer yourself.    In Python, you can use [this function](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html)    The output of the significance test will give you a p-value. If the p-value is less than 0.05 (our chosen significance level), then we will say that there is a significant difference between the two average precision lists. That means that there is less than a 5% chance that the difference in the mean of the AP scores is due to random fluctuation.    Write the p-value in a file called **significance.txt**.   ***Do not include anything else in the file, just this number!***    ## Grading    Your grade will be based on:  - implementing the InL2 parameter correctly -- 0.7 (70) points  - uploading significance.txt with the p-value. -- 0.3 (30) points  
CS410 MP2---Search Engines  In this 4-part MP, you will get familiar with building and evaluating Search Engines.    Part 4  In the final part of MP2, you will participate in a search competition where you will create a Search Engine using MeTA, similar to what you did for Part 2. Your ranker will be evaluated using NDCG scores on 3 relevance datasets: Cranfield dataset, APNews dataset, and the Faculty dataset collected and annotated by you and your classmates.    The evaluation results will be displayed on the leaderboard on LiveDataLab. Only your latest submission results will be displayed here. Also, the grader will use the default config.toml and stopwords.txt files provided here. So, changing these files will not make any impact on the scores. Please modify search_eval.py only to potentially notice any changes in the scores.    Due: Oct 2, 2022 at 11:59 pm CDT  NOTE: If you've completed Part2, you should be familiar with the basics: Setup, Indexing and Searching the Data. We've included these sections in this README again for convenience. So, feel free to skip directly to the Competition Tasks section!    Setup  We'll use metapy---Python bindings for MeTA. If you have not installed metapy so far, use the following commands to get started.    # Ensure your pip is up to date  pip install --upgrade pip    # install metapy!  pip install metapy pytoml  If you're on an EWS machine    module load python3  # install metapy on your local directory  pip install metapy pytoml --user  Read the C++ Search Tutorial. Read Initially setting up the config file and Relevance judgements. Read the python Search Tutorial    If you cloned this repo correctly, your assignment directory should look like this:    MP2_part4/: assignment folder  MP2_part4/cranfield/: Cranfield dataset in MeTA format.  MP2_part4/cranfield-queries.txt: Queries one per line, copy it from the cranfield directory.  MP2_part4/cranfield-qrels.txt: Relevance judgements for the queries, copy it from the cranfield directory.  MP2_part4/stopwords.txt: A file containing stopwords that will not be indexed.  MP2_part4/config.toml: A config file with paths set to all the above files, including index and ranker settings.  Indexing the data  To index the data using metapy, you can use either Python 2 or 3.    import metapy  idx = metapy.index.make_inverted_index('config.toml')  Search the index  You can examine the data inside the cranfield directory to get a sense about the dataset and the queries.    To examine the index we built from the previous section. You can use metapy's functions.    # Examine number of documents  idx.num_docs()  # Number of unique terms in the dataset  idx.unique_terms()  # The average document length  idx.avg_doc_length()  # The total number of terms  idx.total_corpus_terms()  Here is a list of all the rankers in MeTA.Viewing the class comment in the header files shows the optional parameters you can set in the config file:    Okapi BM25, method = bm25  Pivoted Length Normalization, method = pivoted-length  Absolute Discount Smoothing, method = absolute-discount  Jelinek-Mercer Smoothing, method = jelinek-mercer  Dirichlet Prior Smoothing, method = dirichlet-prior  In metapy, the rankers can be called as:    metapy.index.OkapiBM25(k1, b, k3) where k1, b, k3 are function arguments, e.g. ranker = metapy.index.OkapiBM25(k1=1.2,b=0.75,k3=500)  metapy.index.PivotedLength(s)   metapy.index.AbsoluteDiscount(delta)  metapy.index.JelinekMercer(lambda)  metapy.index.DirichletPrior(mu)  Competition Tasks  search_eval.py contains some starter code to evaluate the performance of the OkapiBM25 ranker on the cranfield dataset using NDCG. You should modify this file for the competition.    You are free to use any metapy ranker, fine-tune various parameter settings or even your use your own implementation of ranking functions. Feel free to improvise and create your own rankers! You may use the provided cranfield dataset to evaluate your rankers/parameter settings locally but remember that the leaderboard ranking is based on the performance on all the 3 datasets, so please make sure you do not overfit.    To see how well you perform in the leaderboard, you need to edit the load_ranker function inside search_eval.py to return the ranker of your choice.There are no restrictions on the number of submissions.    Grading  You must beat the baseline on the leaderboard to get full credit, i.e. your Overall Score should be greater than the Overall Score of the baseline. The last column on the Leaderboard indicates whether you completed this requirement or not (1 or 0).    The leaderboard also shows more details of the performance (NDCG@10). Overall Score is computed as 0.1* NDCG@10 on APNews + 0.3* NDCG@10 on Cranfield + 0.6* NDCG@10 on Faculty dataset.    Bonus: Top ranked in Search Competition Leaderboard  Try to get a top position in the competition leaderboard. The higher your rank is, the more extra credit you will receive. The rank is based on Overall Score. Our grading formula for the competition is max(0, 5-(Rank-1)/10) where Rank is the position of the student. This means that about 50 students will receive an extra credit greater than 0. Students with the same score will receive the same extra credit.    You will not immediately see the extra credit score on Coursera since it depends on your final rank in the leaderboard when the deadline is over. We will add your extra credit score on Coursera later.
In MP3, you will implement the probabilistic latent semantic analysis PLSA algorithm discussed in lectures 9.7 and 9.8 of the Text Mining Coursera course. You are not required to implement the probabilistic latent semantic analysis PLSA algorithm with a background model (we will run tests assuming the background model has not been implemented).   MP3: Implementing the PLSA Algorithm  DUE: OCT 23, 2022 at 11:59pm  In this MP, you will implement the PLSA algorithm discussed in lectures 9.7 and 9.8 of the Text Mining Coursera course. You are not required to implement the PLSA algorithm with a background model (we will run tests assuming the background model has not been implemented). You are provided with two data sets in the data folder: test.txt and dblp.txt. You can assume that each line is a separate document.    The test.txt data set contains 1000 lines. Each line is a document. The first 100 documents are labeled with the topic the document belongs to, either 0 (Seattle) or 1 (Chicago). Each of the first 100 document is generated by sampling completely from the topic that is labelled (i.e., generated from one topic only). The rest 900 documents are generated from a mixture model of the topic Seattle and Chicago. Run your code to test if your EM algorithm returns reasonable results.    The DBLP.txt data set is made up of research paper abstracts from DBLP. Each line is a document. Make sure to test your code on the simpler data set test.txt before running it on DBLP.txt.    You are provided with a code skeleton plsa.py. Do not change anything in the def __init__() function. But feel free to change anything in the main() function to test your code.    Some suggested tips:    Using matrices is strongly encouraged (writing nested for-loops for calculation is painful)  Python libraries numpy and scipy are useful in matrix based calculations  Writing the PLSA Algorithm:  The original PLSA model does not contain a background model. This MP also is based on the original PLSA model, you do not have to worry about the background model. However, lectures are all about PLSA with a background model, so you should not attempt to map the formulas on lecture slides directly to the code. Instead, you would need to adjust the formulas on slides by assuming that there is zero probability that the background model would be chosen. That is, you should set λB to zero. If you do this, you will see that the E-step would essentially only compute a distribution over k topics for z=1, ..., k, given each word, i.e., p(z=i|w). The M-step would also be simpler as p(Z=B|w) is also zero for all words (due to the fact that λB=0). If you are still confused, please take a look at Section 3 of Chase Geigle's note on EM [2] and the note below.    The main data structures involved in the implementation of this EM algorithm are three matrices:    T (topics by words): this is the set of parameters characterizing topic content that we denoted by θi's. Each element is the probability of a particular word in a particular topic.    D (documents by topics): this is the set of parameters modeling the coverage of topics in each document, which we denoted by pij's. Each element is the probability of a particular topic is covered in a particular document.    Z (hidden variables): For every document, we need one Z which represents the probability that each word in the document has been generated from a particular topic, so for any document, this is a word-by-topic matrix, encoding p(Z|w) for a particular document. Z is the matrix that we compute in the E-step (based on matrices T and D, which represent our parameters). Note that we need to compute a different Z for each document, so we need to allocate a matrix Z for every document. If we do so, the M-step is simply to use all these Z matrices together with word counts in each document to re-estimate all the parameters, i.e., updating matrices T and D based on Z. Thus at a high level, this is what's happening in the algorithm:    T and D are initialized.  E-step computes all Z's based on T and D.  M-step uses all Z's to update T and D.  We iterate until the likelihood doesn't change much when we would use T and D as our output. Note that Zs are also very useful (can you imagine some applications of Zs?).  Resources:  [1] Cheng’s note on the EM algorithm  [2] Chase Geigle’s note on the EM algorithm, which includes a derivation of the EM algorithm (see section 4), and  [3] Qiaozhu Mei’s note on the EM algorithm for PLSA, which includes a different derivation of the EM algorithm.  